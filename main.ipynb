{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizerFast, BertForSequenceClassification,\n",
    "    XLNetTokenizerFast, XLNetForSequenceClassification\n",
    ")\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & split dataset\n",
    "df = pd.read_csv(\"processed_data.csv\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, stratify=train_df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29474bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model_and_tokenizer(model_type: str, num_labels: int):\n",
    "    \"\"\"\n",
    "    model_type: 'bert' hoặc 'xlnet'\n",
    "    \"\"\"\n",
    "    if model_type == \"bert\":\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", num_labels=num_labels\n",
    "        )\n",
    "    elif model_type == \"xlnet\":\n",
    "        tokenizer = XLNetTokenizerFast.from_pretrained(\n",
    "            \"xlnet-base-cased\", do_lower_case=True\n",
    "        )\n",
    "        model = XLNetForSequenceClassification.from_pretrained(\n",
    "            \"xlnet-base-cased\", num_labels=num_labels\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Chỉ hỗ trợ 'bert' hoặc 'xlnet'\")\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training class\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion,\n",
    "                 train_loader, val_loader, device,\n",
    "                 epochs=5, patience=1):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.history = {\n",
    "            \"train_loss\": [], \"train_acc\": [],\n",
    "            \"val_loss\": [],   \"val_acc\": []\n",
    "        }\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        losses, correct = [], 0\n",
    "        for batch in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch = {k: v.to(self.device) for k,v in batch.items()}\n",
    "            outputs = self.model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                return_dict=True\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = self.criterion(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "\n",
    "        return correct / len(self.train_loader.dataset), np.mean(losses)\n",
    "\n",
    "    def eval_epoch(self, loader):\n",
    "        self.model.eval()\n",
    "        losses, correct = [], 0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = {k: v.to(self.device) for k,v in batch.items()}\n",
    "                logits = self.model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    return_dict=True\n",
    "                ).logits\n",
    "                loss = self.criterion(logits, batch[\"labels\"])\n",
    "                losses.append(loss.item())\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == batch[\"labels\"]).sum().item()\n",
    "        return correct / len(loader.dataset), np.mean(losses)\n",
    "\n",
    "    def train(self):\n",
    "        best_loss = float(\"inf\")\n",
    "        patience_ctr = 0\n",
    "\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            tr_acc, tr_loss = self.train_epoch()\n",
    "            va_acc, va_loss = self.eval_epoch(self.val_loader)\n",
    "\n",
    "            self.history[\"train_acc\"].append(tr_acc)\n",
    "            self.history[\"train_loss\"].append(tr_loss)\n",
    "            self.history[\"val_acc\"].append(va_acc)\n",
    "            self.history[\"val_loss\"].append(va_loss)\n",
    "\n",
    "            print(f\"[{epoch}/{self.epochs}] \"\n",
    "                  f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n",
    "                  f\"Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n",
    "\n",
    "            if va_loss < best_loss:\n",
    "                best_loss = va_loss\n",
    "                patience_ctr = 0\n",
    "                torch.save(self.model.state_dict(), f\"best_{model_type}.pt\")\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "                if patience_ctr > self.patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        # Load best\n",
    "        self.model.load_state_dict(torch.load(f\"best_{model_type}.pt\"))\n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e882fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot helper\n",
    "def plot_history(history, title_prefix=\"\"):\n",
    "    # Loss\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"val_loss\"],   label=\"val_loss\")\n",
    "    plt.title(f\"{title_prefix} Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Acc\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(history[\"val_acc\"],   label=\"val_acc\")\n",
    "    plt.title(f\"{title_prefix} Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcdd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = train_df[\"label\"].nunique()\n",
    "\n",
    "results = {}\n",
    "for model_type in [\"bert\", \"xlnet\"]:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Training & Eval: {model_type.upper()}\")\n",
    "    tokenizer, model = build_model_and_tokenizer(model_type, num_labels)\n",
    "\n",
    "    # DataLoader\n",
    "    train_ds = NewsDataset(train_df[\"text\"], train_df[\"label\"], tokenizer)\n",
    "    val_ds   = NewsDataset(val_df[\"text\"],   val_df[\"label\"],   tokenizer)\n",
    "    test_ds  = NewsDataset(test_df[\"text\"],  test_df[\"label\"],  tokenizer)\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False)\n",
    "\n",
    "    # Optimizer & loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model, optimizer, criterion,\n",
    "        train_loader, val_loader,\n",
    "        device, epochs=5, patience=1\n",
    "    )\n",
    "    history = trainer.train()\n",
    "\n",
    "    # Plot\n",
    "    plot_history(history, title_prefix=model_type.upper())\n",
    "\n",
    "    # Test set\n",
    "    test_acc, test_loss = trainer.eval_epoch(test_loader)\n",
    "    print(f\"Test {model_type}: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
    "\n",
    "    results[model_type] = {\n",
    "        \"history\": history,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_loss\": test_loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference engine\n",
    "class InferenceEngine:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, text, max_length=256):\n",
    "        enc = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\",\n",
    "            max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k,v in enc.items()}\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            conf, pred = probs.max(dim=1)\n",
    "        return pred.item(), conf.item()\n",
    "\n",
    "for model_type in results:\n",
    "    print(\"\\n--- Inference with\", model_type.upper(), \"---\")\n",
    "    tokenizer, _ = build_model_and_tokenizer(model_type, num_labels)\n",
    "    model = build_model_and_tokenizer(model_type, num_labels)[1]\n",
    "    model.load_state_dict(torch.load(f\"best_{model_type}.pt\"))\n",
    "    engine = InferenceEngine(model, tokenizer, device)\n",
    "\n",
    "    samples = test_df.head(10)\n",
    "    for _, row in samples.iterrows():\n",
    "        p, c = engine.predict(row[\"text\"])\n",
    "        print(f\"Text[:50]: {row['text'][:50]:50s} | True: {row['label']} → Pred: {p} (conf={c:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
